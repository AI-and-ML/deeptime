{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-lagged autoencoders\n",
    "\n",
    "For users already familiar with the time-lagged autoencoders interface: The corresponding API docs [TAE](../api/generated/deeptime.decomposition.TAE.rst#deeptime.decomposition.TAE) and [TVAE](../api/generated/deeptime.decomposition.TVAE.rst#deeptime.decomposition.TVAE).\n",
    "\n",
    "Time-lagged autoencoders <cite data-cite=\"nbtae-wehmeyer2018timelagged\">(Wehmeyer, 2018)</cite> are a type of neural network approach which tries to first compress / encode instantaneous data through a function\n",
    "\n",
    "$$ E : \\mathbb{R}^N \\to \\mathbb{R}^n, x_t\\mapsto E(x_t) $$\n",
    "\n",
    "with $N \\geq n$ and then reconstruct $x_{t+\\tau}$ from $E(x_t)$ through a decoder network\n",
    "\n",
    "$$ D : \\mathbb{R}^n \\to \\mathbb{R}^N, z \\mapsto D(z). $$\n",
    "\n",
    "To this end, the optimization target is defined as the mean-squared error between $x_{ t + \\tau }$ and $D(E( x_t ))$.\n",
    "\n",
    "By this, they differ from classical autoencoders which would try to reconstruct the instantaneous data.\n",
    "\n",
    "In deeptime, time-lagged autoencoders come in two flavors, analogously to autoencoders and variational autoencoders <cite data-cite=\"nbtae-kingma2013auto\">(Kingma, 2013)</cite>.\n",
    "To get started, we need to import [PyTorch](https://pytorch.org/) as well as deeptime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptime as dt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we need to specify how many CPU threads to use and on what kind of device to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_num_threads(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This documentation explains the basic API of time-lagged autoencoders using sqrt-model data. It is a hidden two-state jump process with a two-dimensional observable emission distribution so that the two states cannot be linearly separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtraj, traj = dt.data.sqrt_model(n_samples=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `dtraj` are the discrete reference states and `traj` is the observable trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, Y = np.meshgrid(\n",
    "    np.linspace(np.min(traj[:, 0]), np.max(traj[:, 0]), 100),\n",
    "    np.linspace(np.min(traj[:, 1]), np.max(traj[:, 1]), 100),\n",
    ")\n",
    "kde_input = np.dstack((X, Y)).reshape(-1, 2)\n",
    "\n",
    "kernel = stats.gaussian_kde(traj.T, bw_method=.1)\n",
    "Z = kernel(kde_input.T).reshape(X.shape)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "ax1.plot(dtraj[:500])\n",
    "ax1.set_title('Discrete trajectory')\n",
    "ax1.set_xlabel('time (a.u.)')\n",
    "ax1.set_ylabel('state')\n",
    "\n",
    "cm = ax2.contourf(X, Y, Z)\n",
    "plt.colorbar(cm, ax=ax2);\n",
    "ax2.set_title('Heatmap of observations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make life downstream a bit easier, we wrap the data into a [time-lagged dataset](../api/generated/deeptime.data.TimeLaggedDataset.rst#deeptime.data.TimeLaggedDataset), perform a train/validation set split, and create PyTorch data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = dt.data.TimeLaggedDataset.from_trajectory(lagtime=1, data=traj.astype(np.float32))\n",
    "\n",
    "n_val = int(len(dataset)*.5)\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [len(dataset) - n_val, n_val])\n",
    "\n",
    "loader_train = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "loader_val = DataLoader(val_data, batch_size=len(val_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "To define an ordinary autoencoder, encoder and decoder torch modules must be provided. To make things easier, deeptime offers the multi-layer preceptron as a building block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeptime.util.torch import MLP\n",
    "\n",
    "units = [2, 10, 10, 1]\n",
    "encoder = MLP(units, nonlinearity=torch.nn.ReLU, output_nonlinearity=torch.nn.Sigmoid, \n",
    "              initial_batchnorm=False)\n",
    "decoder = MLP(units[::-1], nonlinearity=torch.nn.ReLU, initial_batchnorm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that is left to do is create a time-lagged autoencoder estimator and fit it on the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tae = dt.decomposition.TAE(encoder, decoder, learning_rate=1e-3)\n",
    "tae.fit(loader_train, n_epochs=30, validation_loader=loader_val)\n",
    "tae_model = tae.fetch_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss can be visualized and it can be see, that our model does not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(*tae.train_losses.T, label='train')\n",
    "plt.semilogy(*tae.validation_losses.T, label='validation')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection reveals a separation into two states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = tae_model.transform(traj)\n",
    "plt.plot(proj[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coloring the data points according to the latent code reveals that the model managed to disentangle the sqrt-model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.scatter(*traj.T, c=proj, cmap='coolwarm')\n",
    "plt.colorbar(cm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoders\n",
    "\n",
    "Instead of learning a deterministic forward mapping, in time-lagged variational autoencoders (TVAEs) a input distribution is mapped to a target distribution. To this end, the encoder maps to Gaussians and the decoder transforms the Gaussian distribution to the desired output distribution.\n",
    "\n",
    "Since Gaussians are parameterized, the encoder maps not to a single latent code but to a mean vector and a log-variance vector, i.e., is expected to produce a tuple of tensors.\n",
    "\n",
    "Again, deeptime provides a multilayer-perceptron-like architecture that performs this job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = dt.decomposition.TVAEEncoder([2, 100, 100, 1], nonlinearity=torch.nn.ReLU)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder can be an ordinary MLP as in the autoencoder case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = MLP([1, 100, 100, 2], nonlinearity=torch.nn.ReLU, initial_batchnorm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the TVAE can be constructed and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvae = dt.decomposition.TVAE(encoder, decoder, learning_rate=1e-3)\n",
    "tvae.fit(loader_train, n_epochs=30, validation_loader=loader_val)\n",
    "tvae_model = tvae.fetch_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we can have a look at the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(*tvae.train_losses.T, label='train')\n",
    "plt.semilogy(*tvae.validation_losses.T, label='validation')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, the network separates the two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plt.scatter(*traj.T, c=tvae_model.transform(traj), cmap='coolwarm')\n",
    "plt.colorbar(cm);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. rubric:: References\n",
    "\n",
    ".. bibliography:: /references.bib\n",
    "    :style: unsrt\n",
    "    :filter: docname in docnames\n",
    "    :keyprefix: nbtae-"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
