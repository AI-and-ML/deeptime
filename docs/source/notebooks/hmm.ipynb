{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "For users already familiar with the interface, the [API docs](../api/index_markov.rst#sktime-markov-hmm).\n",
    "\n",
    "Hidden markov models (HMM) are a type of markov model where the underlying markov process $X_t$ is _hidden_ and there is an _observable_ process $Y_t$ which depends on $X_t$. When fitting such a model, the goal is to parameterize a markov model over $X_t$ by observing $Y_t$. Here the dependency of $Y_t$ on $X_t$ must be immediate, i.e.,\n",
    "\n",
    "$$ \\mathbb{P}(Y_t \\in A \\mid X_1 = x_1, \\ldots, X_n = x_n) = \\mathbb{P}(Y_t \\in A \\mid X_n = x_n). $$\n",
    "\n",
    "The states of process $X_i$ are often called \"hidden states\" or \"macro states\", whereas $\\mathbb{P}(Y_t\\in A \\mid X_n = x_n)$ is referred to as output probability or emission probability.\n",
    "\n",
    "In particular that means that - opposed to ordinary [markov state models](mlmsm.ipynb), here one hidden state maps to a probability distribution over the observable space in which $Y_t$ takes its values.\n",
    "\n",
    "Due to its higher-order nature, it can recover from situations and produce good results when MSMs would not, for example in some cases of bad state space discretization. However, HMMs are harder to fit, computationally more expensive, and the estimation likes to get stuck in local optima.\n",
    "\n",
    "In scikit-time, a hidden markov model is determined by\n",
    "\n",
    "- a hidden transition model $P\\in\\mathbb{R}^{n\\times n}$ (of type [MarkovStateModel](../api/generated/sktime.markov.msm.MarkovStateModel.rst#sktime.markov.msm.MarkovStateModel)) describing the hidden process,\n",
    "- an [output model](../api/generated/sktime.markov.hmm.OutputModel.rst#sktime.markov.hmm.OutputModel), which holds information on the emission probabilities,\n",
    "- and an initial distribution $\\pi\\in\\mathbb{R}^n$ over the hidden states, which expresses a prior belief over the average most likely distribution of hidden states.\n",
    "\n",
    "Creating an [HMM](../api/generated/sktime.markov.hmm.HiddenMarkovStateModel.rst#sktime.markov.hmm.HiddenMarkovStateModel) is as simple as importing sktime and fixing at least the transition model and the output model.\n",
    "\n",
    "Here $P\\in\\mathbb{R}^{2\\times 2}$ is the hidden transition matrix and $E\\in\\mathbb{R}^{2\\times 3}$ are the emission probabilities mapping from two hidden states to two three-state output probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sktime\n",
    "import sktime.markov.hmm as hmm\n",
    "\n",
    "P = np.array([[0.95, 0.05], [0.3, 0.7]])\n",
    "E = np.array([\n",
    "    [0.1, 0.1, 0.8], [0.5, 0.5, 0.]\n",
    "])\n",
    "\n",
    "ground_truth = hmm.HiddenMarkovStateModel(P, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hidden markov model has two hidden (macro) states and three output (micro) states. The initial distribution defaults to a uniform distribution, which prefers no hidden state over the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# hidden states:\", ground_truth.n_hidden_states)\n",
    "print(\"# observation states:\", ground_truth.n_observation_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate an hidden and corresponding observation sequence from the hmm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hidden_trajectory, observation_trajectory) = ground_truth.simulate(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the hidden trajectory, one can observe that state 0 is preferred. The observation trajectory jumps between three states. If the hidden state is $0$, the observation trajectory is most of the time in micro state $2$, otherwise it is in state $0$ or $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(hidden_trajectory[:200])\n",
    "ax1.set_ylabel(\"macrostate\")\n",
    "ax2.plot(observation_trajectory[:200])\n",
    "ax2.set_xlabel(\"time\")\n",
    "ax2.set_ylabel(\"microstate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output probabilities in this case map to discrete values, this is not the only option. We implement\n",
    "\n",
    "- the [discrete output model](../api/generated/sktime.markov.hmm.DiscreteOutputModel.rst#sktime.markov.hmm.DiscreteOutputModel), which maps to a discrete observable space and is parameterized by a row-stochastic output probability matrix $E\\in\\mathbb{R}^{n\\times m}$, where $n$ is the number hidden and $m$ the number of observable states,\n",
    "- and the [gaussian output model](../api/generated/sktime.markov.hmm.GaussianOutputModel.rst#sktime.markov.hmm.GaussianOutputModel), which maps to a one-dimensional output space with $m$ Gaussians, each parameterized by mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "The estimation of an HMM given sequences of observations can be performed using the Baum-Welch algorithm <cite data-cite=\"nbhmm-baum1967inequality\">(Baum, 1967)</cite>, which belongs to the family of expectation-maximization algorithms.\n",
    "\n",
    "It crucially requires an initial guess for emission probabilities and hidden state transition matrix, as the optimization is prone to getting stuck in local optima. After optimization, the likelihood for a trained model is available and can be compared to models with different initial guesses.\n",
    "\n",
    "Going back to above example, we can try to fit a HMM based on the observation trajectory for different initial guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_est_real = hmm.MaximumLikelihoodHMSM(\n",
    "    initial_model=ground_truth\n",
    ").fit(observation_trajectory).fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum absolute deviation: {:.3f}\".format(\n",
    "    np.max(np.abs(hmm_est_real.transition_model.transition_matrix - P))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initialized with the ground truth, the deviation is not large from the initial model. Scikit-time furthermore offers some initialization heuristics should the ground truth be unknown.\n",
    "\n",
    "For discrete output models:\n",
    "\n",
    "1. [init.discrete.metastable_from_data](../api/generated/sktime.markov.hmm.init.discrete.metastable_from_data.rst#sktime.markov.hmm.init.discrete.metastable_from_data) or [init.discrete.metastable_from_msm](../api/generated/sktime.markov.hmm.init.discrete.metastable_from_msm.rst#sktime.markov.hmm.init.discrete.metastable_from_msm): A reversible [MSM](mlmsm.ipynb) is estimated from given discrete trajectories. This is optional in case the MSM is already available. Then, the estimated transition matrix $P\\in\\mathbb{R}^{n\\times n}$ is coarse-grained with PCCA+ <cite data-cite=\"nbhmm-roblitz2013fuzzy\">(Röblitz, 2013)</cite>, yielding $m$ hidden states and a membership matrix $M\\in\\mathbb{R}^{n\\times m}$. The transition matrix is projected into hidden space via\n",
    "    $$ P_\\mathrm{coarse} = (M^\\top M)^{-1}M^\\top PM. $$\n",
    "    This procedure is described in greater detail in <cite data-cite=\"nbhmm-noe2013projected\">(Noé, 2013)</cite>.\n",
    "2. [init.discrete.random](../api/generated/sktime.markov.hmm.init.discrete.random.rst#sktime.markov.hmm.init.discrete.random): Initializes hidden transition matrix and initial distribution uniformly, the emission probabilities are drawn from $\\mathcal{U}(0, 1)$ and normalized to form a row-stochastic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a random initial model and use it as starting point for the maximum-likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_hmm_random = hmm.init.discrete.random(n_observation_states=3, n_hidden_states=2)\n",
    "hmm_est_random = hmm.MaximumLikelihoodHMSM(\n",
    "    initial_model=init_hmm_random\n",
    ").fit(observation_trajectory).fetch_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here, we can compute the maximum deviation from ground truth hidden transition matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum absolute deviation: {:.3f}\".format(\n",
    "    np.max(np.abs(hmm_est_random.transition_model.transition_matrix - P))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian sampling\n",
    "\n",
    "Bayesian sampling of HMMs can be used to obtain confidences. In particular, Gibbs sampling can be performed around a reference HMM, as introduced in <cite data-cite=\"nbhmm-chodera2011bayesian\">(Chodera, 2011)</cite>. In scikit-time, a [BayesianHMSM](../api/generated/sktime.markov.hmm.BayesianHMSM.rst#sktime.markov.hmm.BayesianHMSM) estimator is provided.\n",
    "\n",
    "The easiest method to obtain a BayesianHMSM estimator from data is the [default()](../api/generated/sktime.markov.hmm.BayesianHMSM.rst#sktime.markov.hmm.BayesianHMSM.default) static method. It estimates an ML-MSM on data, coarse-grains it to the requested number of hidden states and uses it as starting point for an ML-HMSM estimation. The resulting model is used as start sample for the BayesianHMSM estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = hmm.BayesianHMSM.default(observation_trajectory, n_hidden_states=2, lagtime=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum-likelihood estimation is done for fewer iterations and with less precision than the default settings, as a rough estimate suffices to start sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum absolute deviation: {:.3f}\".format(\n",
    "    np.max(np.abs(estimator.initial_hmm.transition_model.transition_matrix - P))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `fit` on the estimator starts sampling and yields a [BayesianHMMPosterior](../api/generated/sktime.markov.hmm.BayesianHMMPosterior.rst#sktime.markov.hmm.BayesianHMMPosterior) instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = estimator.fit(\n",
    "    observation_trajectory, \n",
    "    n_burn_in=100,  # number of sampling steps to discard at the beginning \n",
    "    n_thin=10  # take every 10th sample\n",
    ").fetch_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any quantity that can be evaluated on the hidden markov state model object can also be accessed through the posterior but equipped with uncertainties. For this, call `gather_stats` with the name of the method or quantity that should be evaluated. If the method requires arguments, they can also be provided. If it is a method of a lower-level object, it can be separated by slashes `/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = posterior.gather_stats(\"transition_model/transition_matrix\", confidence=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the confidence of, e.g., the transition matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0, 1, 2, 3]\n",
    "y = stats.mean.flatten()\n",
    "errors = np.concatenate((stats.mean - stats.L, -stats.mean + stats.R)).T\n",
    "f, ax = plt.subplots()\n",
    "ax.errorbar(x, y, xerr=errors, fmt='o', color=\"k\")\n",
    "ax.annotate(r\"$P_{11}$\", (x[0]+.05, y[0]+.05))\n",
    "ax.annotate(r\"$P_{12}$\", (x[1]+.05, y[1]+.05))\n",
    "ax.annotate(r\"$P_{21}$\", (x[2]+.05, y[2]+.05))\n",
    "ax.annotate(r\"$P_{22}$\", (x[3]-.1, y[3]-.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, it is possible to instantiate a bayesian HMSM estimator directly by using a previously estimated HMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = hmm.BayesianHMSM(initial_hmm=posterior.prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform sampling as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = estimator.fit(\n",
    "    observation_trajectory, \n",
    "    n_burn_in=100,  # number of sampling steps to discard at the beginning \n",
    "    n_thin=10  # take every 10th sample\n",
    ").fetch_model()\n",
    "stats = posterior.gather_stats(\"transition_model/transition_matrix\", confidence=0.95)\n",
    "\n",
    "x = [0, 1, 2, 3]\n",
    "y = stats.mean.flatten()\n",
    "errors = np.concatenate((stats.mean - stats.L, -stats.mean + stats.R)).T\n",
    "f, ax = plt.subplots()\n",
    "ax.errorbar(x, y, xerr=errors, fmt='o', color=\"k\")\n",
    "ax.annotate(r\"$P_{11}$\", (x[0]+.05, y[0]+.05))\n",
    "ax.annotate(r\"$P_{12}$\", (x[1]+.05, y[1]+.05))\n",
    "ax.annotate(r\"$P_{21}$\", (x[2]+.05, y[2]+.05))\n",
    "ax.annotate(r\"$P_{22}$\", (x[3]-.1, y[3]-.1));"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. rubric:: References\n",
    "\n",
    ".. bibliography:: /references.bib\n",
    "    :style: unsrt\n",
    "    :filter: docname in docnames\n",
    "    :keyprefix: nbhmm-"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
