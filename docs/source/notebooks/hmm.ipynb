{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models\n",
    "\n",
    "For users already familiar with the interface, the [API docs](../api/index_markov_hmm.rst).\n",
    "\n",
    "Hidden Markov models (HMM) are a type of Markov model where the underlying Markov process $X_t$ is _hidden_ and there is an _observable_ process $Y_t$ which depends on $X_t$. A nice introduction into HMM theory and related algorithms can be found in <cite data-cite=\"nbhmm-rabiner1989tutorial\">(Rabiner, 1989)</cite>. When fitting such a model, the goal is to parameterize a Markov model over $X_t$ by observing $Y_t$. Here the dependency of $Y_t$ on $X_t$ must only depend on the current time step, i.e.,\n",
    "\n",
    "$$ \\mathbb{P}(Y_t \\in A \\mid X_1 = x_1, \\ldots, X_n = x_n) = \\mathbb{P}(Y_t \\in A \\mid X_n = x_n), $$\n",
    "\n",
    "where $A$ is some measurable set in observation space.\n",
    "\n",
    "The states of process $X_i$ are often called \"hidden states\" or \"macro states\", whereas $\\mathbb{P}(Y_t\\in A \\mid X_n = x_n)$ is referred to as output probability or emission probability. If the emission probability is a discrete probability distribution, the corresponding states are often called \"observable\" or \"micro\" states.\n",
    "\n",
    "In particular, this means that here - as opposed to in ordinary [Markov state models](mlmsm.ipynb) - one hidden state maps to a probability distribution over the observable space in which $Y_t$ takes its values.\n",
    "\n",
    "HMMs are generally more expressive as MSMs, as each MSM can be written in terms of an HMM with discrete output probabilities: The transition matrix can be used as hidden transition matrix and the emission probabilities reduce to the identity, i.e., there are as many hidden states as there are observable states and each hidden state maps to a delta distribution. HMMs relax this by allowing for other distributions too.\n",
    "\n",
    "Due to their more general nature, HMMs can recover from situations and produce good results when MSMs would not, for example in some cases of bad state space discretization. However, HMMs are harder to fit, computationally more expensive, and the estimation likes to get stuck in local optima.\n",
    "\n",
    "In deeptime, a hidden Markov model is determined by\n",
    "\n",
    "- a hidden transition model $P\\in\\mathbb{R}^{n\\times n}$ (of type [MarkovStateModel](../api/generated/deeptime.markov.msm.MarkovStateModel.rst#deeptime.markov.msm.MarkovStateModel)) describing the hidden process,\n",
    "- an [output model](../api/generated/deeptime.markov.hmm.OutputModel.rst#deeptime.markov.hmm.OutputModel), which holds information on the emission probabilities,\n",
    "- and an initial distribution $\\pi\\in\\mathbb{R}^n$ over the hidden states, which expresses a prior belief over the average most likely distribution of hidden states.\n",
    "\n",
    "Creating an [HMM](../api/generated/deeptime.markov.hmm.HiddenMarkovModel.rst#deeptime.markov.hmm.HiddenMarkovModel) is as simple as importing deeptime and fixing at least the transition model and the output model.\n",
    "\n",
    "Here $P\\in\\mathbb{R}^{2\\times 2}$ is the hidden transition matrix and $E\\in\\mathbb{R}^{2\\times 3}$ are the emission probabilities mapping from two hidden states to two three-state output probability distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:33.764020Z",
     "iopub.status.busy": "2020-10-26T11:59:33.762199Z",
     "iopub.status.idle": "2020-10-26T11:59:34.521874Z",
     "shell.execute_reply": "2020-10-26T11:59:34.521388Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deeptime\n",
    "import deeptime.markov.hmm as hmm\n",
    "\n",
    "P = np.array([[0.95, 0.05], [0.3, 0.7]])\n",
    "E = np.array([\n",
    "    [0.1, 0.1, 0.8], [0.5, 0.5, 0.]\n",
    "])\n",
    "\n",
    "ground_truth = hmm.HiddenMarkovModel(P, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hidden Markov model has two hidden (macro) states and three output (micro) states. The initial distribution defaults to a uniform distribution, which prefers no hidden state over the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:34.525026Z",
     "iopub.status.busy": "2020-10-26T11:59:34.524639Z",
     "iopub.status.idle": "2020-10-26T11:59:34.526656Z",
     "shell.execute_reply": "2020-10-26T11:59:34.526969Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"# hidden states:\", ground_truth.n_hidden_states)\n",
    "print(\"# observation states:\", ground_truth.n_observation_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate an hidden and corresponding observation sequence from the hmm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:34.531903Z",
     "iopub.status.busy": "2020-10-26T11:59:34.531513Z",
     "iopub.status.idle": "2020-10-26T11:59:34.533241Z",
     "shell.execute_reply": "2020-10-26T11:59:34.533691Z"
    }
   },
   "outputs": [],
   "source": [
    "(hidden_trajectory, observation_trajectory) = ground_truth.simulate(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the hidden trajectory, one can observe that state 0 is preferred. The observation trajectory jumps between three states. If the hidden state is $0$, the observation trajectory is most of the time in micro state $2$, otherwise it is in state $0$ or $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:34.539661Z",
     "iopub.status.busy": "2020-10-26T11:59:34.539194Z",
     "iopub.status.idle": "2020-10-26T11:59:35.329182Z",
     "shell.execute_reply": "2020-10-26T11:59:35.329659Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure(constrained_layout=False, figsize=(7, 7))\n",
    "gs = f.add_gridspec(nrows=4, ncols=1)\n",
    "ax1 = f.add_subplot(gs[0, 0])\n",
    "ax2 = f.add_subplot(gs[1, 0])\n",
    "ax3 = f.add_subplot(gs[2:, 0])\n",
    "\n",
    "ax1.plot(hidden_trajectory[:200])\n",
    "ax1.set_ylabel(\"macrostate\")\n",
    "ax2.plot(observation_trajectory[:200])\n",
    "ax2.set_xlabel(\"time\")\n",
    "ax2.set_ylabel(\"microstate\")\n",
    "\n",
    "probabilities = np.zeros((2, 3))\n",
    "for i_hidden in range(2):\n",
    "    for i_obs in range(3):\n",
    "        # indices where macro state is i_hidden\n",
    "        hidden_ix = np.argwhere(hidden_trajectory == i_hidden).flatten()\n",
    "        subselected_obs = observation_trajectory[hidden_ix]\n",
    "        # indices where micro state is i_obs given i_hidden\n",
    "        obs_ix = np.argwhere(subselected_obs == i_obs).flatten()\n",
    "        probabilities[i_hidden, i_obs] = len(obs_ix) / len(hidden_ix)\n",
    "\n",
    "cb = ax3.matshow(probabilities, cmap=\"Reds\")\n",
    "ax3.xaxis.set_ticks_position('bottom')\n",
    "ax3.set_xlabel(\"micro state\")\n",
    "ax3.set_ylabel(\"macro state\")\n",
    "ax3.set_title(r\"conditional probabilities $P(Y_t\\in s_\\mathrm{micro}\\mid X_t\\in s_\\mathrm{macro})$\")\n",
    "f.colorbar(cb, ax=ax3)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output probabilities in this case map to discrete values, this is not the only option. We implement\n",
    "\n",
    "- the [discrete output model](../api/generated/deeptime.markov.hmm.DiscreteOutputModel.rst#deeptime.markov.hmm.DiscreteOutputModel), which maps to a discrete observable space and is parameterized by a row-stochastic output probability matrix $E\\in\\mathbb{R}^{n\\times m}$, where $n$ is the number hidden and $m$ the number of observable states,\n",
    "- and the [Gaussian output model](../api/generated/deeptime.markov.hmm.GaussianOutputModel.rst#deeptime.markov.hmm.GaussianOutputModel), which maps to a one-dimensional output space with $m$ Gaussians, each parameterized by mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "The estimation of an HMM given sequences of observations can be performed using the Baum-Welch algorithm <cite data-cite=\"nbhmm-baum1967inequality\">(Baum, 1967)</cite>, which belongs to the family of expectation-maximization algorithms.\n",
    "\n",
    "It crucially requires an initial guess for emission probabilities and hidden state transition matrix, as the optimization is prone to getting stuck in local optima. After optimization, the likelihood for a trained model is available and can be compared to models with different initial guesses.\n",
    "\n",
    "Going back to above example, we can try to fit an HMM based on the observation trajectory for different initial guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:35.333826Z",
     "iopub.status.busy": "2020-10-26T11:59:35.333327Z",
     "iopub.status.idle": "2020-10-26T11:59:35.351542Z",
     "shell.execute_reply": "2020-10-26T11:59:35.351038Z"
    }
   },
   "outputs": [],
   "source": [
    "hmm_est_real = hmm.MaximumLikelihoodHMM(\n",
    "    initial_model=ground_truth\n",
    ").fit(observation_trajectory).fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:35.354436Z",
     "iopub.status.busy": "2020-10-26T11:59:35.353984Z",
     "iopub.status.idle": "2020-10-26T11:59:35.356546Z",
     "shell.execute_reply": "2020-10-26T11:59:35.355876Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Maximum absolute deviation: {:.3f}\".format(\n",
    "    np.max(np.abs(hmm_est_real.transition_model.transition_matrix - P))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initialized with the ground truth, the deviation is not large from the initial model. Deeptime furthermore offers some initialization heuristics should the ground truth be unknown.\n",
    "\n",
    "For discrete output models:\n",
    "\n",
    "1. [init.discrete.metastable_from_data](../api/generated/deeptime.markov.hmm.init.discrete.metastable_from_data.rst#deeptime.markov.hmm.init.discrete.metastable_from_data) or [init.discrete.metastable_from_msm](../api/generated/deeptime.markov.hmm.init.discrete.metastable_from_msm.rst#deeptime.markov.hmm.init.discrete.metastable_from_msm): A reversible [MSM](mlmsm.ipynb) is estimated from given discrete trajectories. This is optional in case the MSM is already available. Then, the estimated transition matrix $P\\in\\mathbb{R}^{n\\times n}$ is coarse-grained with PCCA+ <cite data-cite=\"nbhmm-roblitz2013fuzzy\">(Röblitz, 2013)</cite>, yielding $m$ metastable/macro states and a membership matrix $M\\in\\mathbb{R}^{n\\times m}$. The transition matrix is projected into hidden space via\n",
    "    $$ P_\\mathrm{coarse} = (M^\\top M)^{-1}M^\\top PM. $$\n",
    "    This procedure is described in greater detail in <cite data-cite=\"nbhmm-noe2013projected\">(Noé, 2013)</cite>.\n",
    "2. [init.discrete.random_guess](../api/generated/deeptime.markov.hmm.init.discrete.random_guess.rst#deeptime.markov.hmm.init.discrete.random_guess): Initializes hidden transition matrix and initial distribution uniformly, the emission probabilities are drawn from the uniform distribution $\\mathcal{U}(0, 1)$ and normalized to form a row-stochastic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a random initial model and use it as starting point for the maximum-likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:35.360771Z",
     "iopub.status.busy": "2020-10-26T11:59:35.360013Z",
     "iopub.status.idle": "2020-10-26T11:59:35.365383Z",
     "shell.execute_reply": "2020-10-26T11:59:35.365845Z"
    }
   },
   "outputs": [],
   "source": [
    "init_hmm_random = hmm.init.discrete.random_guess(n_observation_states=3, n_hidden_states=2)\n",
    "hmm_est_random = hmm.MaximumLikelihoodHMM(\n",
    "    initial_model=init_hmm_random\n",
    ").fit(observation_trajectory).fetch_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here, we can compute the maximum deviation from ground truth hidden transition matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:35.368901Z",
     "iopub.status.busy": "2020-10-26T11:59:35.368440Z",
     "iopub.status.idle": "2020-10-26T11:59:35.370411Z",
     "shell.execute_reply": "2020-10-26T11:59:35.370868Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Maximum absolute deviation: {:.3f}\".format(\n",
    "    np.max(np.abs(hmm_est_random.transition_model.transition_matrix - P))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian sampling\n",
    "\n",
    "Bayesian sampling of HMMs can be used to obtain confidences. In particular, Gibbs sampling can be performed around a reference HMM, as introduced in <cite data-cite=\"nbhmm-chodera2011bayesian\">(Chodera, 2011)</cite>. In deeptime, a [BayesianHMM](../api/generated/deeptime.markov.hmm.BayesianHMM.rst#deeptime.markov.hmm.BayesianHMM) estimator is provided.\n",
    "\n",
    "The easiest method to obtain a BayesianHMM estimator from data is the [default()](../api/generated/deeptime.markov.hmm.BayesianHMM.rst#deeptime.markov.hmm.BayesianHMM.default) static method. It estimates an ML-MSM on data, coarse-grains it to the requested number of hidden states and uses it as starting point for an ML-HMM estimation. The resulting model is used as start sample for the BayesianHMM estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:35.373707Z",
     "iopub.status.busy": "2020-10-26T11:59:35.373248Z",
     "iopub.status.idle": "2020-10-26T11:59:35.453963Z",
     "shell.execute_reply": "2020-10-26T11:59:35.454440Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator = hmm.BayesianHMM.default(observation_trajectory, n_hidden_states=2, lagtime=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum-likelihood estimation is done for fewer iterations and with less precision than the default settings, as a rough estimate suffices to start sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:35.457447Z",
     "iopub.status.busy": "2020-10-26T11:59:35.456991Z",
     "iopub.status.idle": "2020-10-26T11:59:35.459455Z",
     "shell.execute_reply": "2020-10-26T11:59:35.458974Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Maximum absolute deviation: {:.3f}\".format(\n",
    "    np.max(np.abs(estimator.initial_hmm.transition_model.transition_matrix - P))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `fit` on the estimator starts sampling and yields a [BayesianHMMPosterior](../api/generated/deeptime.markov.hmm.BayesianHMMPosterior.rst#deeptime.markov.hmm.BayesianHMMPosterior) instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:35.462915Z",
     "iopub.status.busy": "2020-10-26T11:59:35.462427Z",
     "iopub.status.idle": "2020-10-26T11:59:38.836255Z",
     "shell.execute_reply": "2020-10-26T11:59:38.836742Z"
    }
   },
   "outputs": [],
   "source": [
    "posterior = estimator.fit(\n",
    "    observation_trajectory, \n",
    "    n_burn_in=100,  # number of sampling steps to discard at the beginning \n",
    "    n_thin=10  # take every 10th sample\n",
    ").fetch_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any quantity that can be evaluated on the hidden Markov state model object can also be accessed through the posterior but equipped with uncertainties. For this, call `gather_stats` with the name of the method or quantity that should be evaluated. If the method requires arguments, they can also be provided. If it is a method of a lower-level object, it can be separated by slashes `/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:38.839771Z",
     "iopub.status.busy": "2020-10-26T11:59:38.839316Z",
     "iopub.status.idle": "2020-10-26T11:59:38.842749Z",
     "shell.execute_reply": "2020-10-26T11:59:38.842263Z"
    }
   },
   "outputs": [],
   "source": [
    "stats = posterior.gather_stats(\"transition_model/transition_matrix\", confidence=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the confidence of, e.g., the transition matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:38.864392Z",
     "iopub.status.busy": "2020-10-26T11:59:38.859981Z",
     "iopub.status.idle": "2020-10-26T11:59:39.001872Z",
     "shell.execute_reply": "2020-10-26T11:59:39.001398Z"
    }
   },
   "outputs": [],
   "source": [
    "x = [0, 1, 2, 3]\n",
    "y = stats.mean.flatten()\n",
    "errors = np.concatenate((stats.mean - stats.L, -stats.mean + stats.R)).T\n",
    "f, ax = plt.subplots()\n",
    "ax.errorbar(x, y, xerr=errors, fmt='o', color=\"k\")\n",
    "ax.annotate(r\"$P_{11}$\", (x[0]+.05, y[0]+.05))\n",
    "ax.annotate(r\"$P_{12}$\", (x[1]+.05, y[1]+.05))\n",
    "ax.annotate(r\"$P_{21}$\", (x[2]+.05, y[2]+.05))\n",
    "ax.annotate(r\"$P_{22}$\", (x[3]-.1, y[3]-.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, it is possible to instantiate a Bayesian HMM estimator directly by using a previously estimated HMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:39.005016Z",
     "iopub.status.busy": "2020-10-26T11:59:39.004641Z",
     "iopub.status.idle": "2020-10-26T11:59:39.007012Z",
     "shell.execute_reply": "2020-10-26T11:59:39.006610Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator = hmm.BayesianHMM(initial_hmm=posterior.prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform sampling as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-26T11:59:39.012761Z",
     "iopub.status.busy": "2020-10-26T11:59:39.012297Z",
     "iopub.status.idle": "2020-10-26T11:59:42.574211Z",
     "shell.execute_reply": "2020-10-26T11:59:42.573879Z"
    }
   },
   "outputs": [],
   "source": [
    "posterior = estimator.fit(\n",
    "    observation_trajectory, \n",
    "    n_burn_in=100,  # number of sampling steps to discard at the beginning \n",
    "    n_thin=10  # take every 10th sample\n",
    ").fetch_model()\n",
    "stats = posterior.gather_stats(\"transition_model/transition_matrix\", confidence=0.95)\n",
    "\n",
    "x = [0, 1, 2, 3]\n",
    "y = stats.mean.flatten()\n",
    "errors = np.concatenate((stats.mean - stats.L, -stats.mean + stats.R)).T\n",
    "f, ax = plt.subplots()\n",
    "ax.errorbar(x, y, xerr=errors, fmt='o', color=\"k\")\n",
    "ax.annotate(r\"$P_{11}$\", (x[0]+.05, y[0]+.05))\n",
    "ax.annotate(r\"$P_{12}$\", (x[1]+.05, y[1]+.05))\n",
    "ax.annotate(r\"$P_{21}$\", (x[2]+.05, y[2]+.05))\n",
    "ax.annotate(r\"$P_{22}$\", (x[3]-.1, y[3]-.1));"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. rubric:: References\n",
    "\n",
    ".. bibliography:: /references.bib\n",
    "    :style: unsrt\n",
    "    :filter: docname in docnames\n",
    "    :keyprefix: nbhmm-"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
